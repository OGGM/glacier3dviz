{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glacier3dviz workflow #1: Hi-res simulation and data creation  \n",
    "This notebook can be used to run high-resolution future simulations of glaciers with the [OPEN GLOBAL GLACIER MODEL(OGGM)](https://oggm.org/). Each single glacier run is dynamically calibrated with OGGM's [dynamic spinup](https://docs.oggm.org/en/stable/dynamic-spinup.html) and can then be forced with different global climate(GCM) model runs.\n",
    "The aim of this notebook is to prepare data for a 3D animation with the [glacier3dviz](https://glacier3dviz.oggm.org/) tool. Therefor the resulting flowline data of the simulaton is redistributed into spatially distributed data which then can be animated on the actual topography in which the glacier is embedded. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from oggm import cfg, utils, workflow, tasks, DEFAULT_BASE_URL\n",
    "from oggm.sandbox import distribute_2d\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import xarray as xr\n",
    "from oggm.shop import gcm_climate\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import shutil"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This series of notebooks is written to be used on a HPC cluster with the help of jupytext and slurm. When run as a notebook, it only executes the workflow as a test-run for one glacier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to detect if we're running in a Jupyter notebook\n",
    "def check_if_notebook():\n",
    "    try:\n",
    "        shell_name = get_ipython().__class__.__name__\n",
    "        if shell_name == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or JupyterLab\n",
    "        elif shell_name in ['TerminalInteractiveShell', 'InteractiveShell']:\n",
    "            return False  # IPython terminal or other interactive shells\n",
    "        else:\n",
    "            # Fallback or default behavior for unidentified environments\n",
    "            return False\n",
    "    except NameError:\n",
    "        return False      # Not in IPython, likely standard Python interpreter\n",
    "\n",
    "# Use this to conditionally execute tests/debugging\n",
    "if check_if_notebook():\n",
    "    is_notebook = True\n",
    "else:\n",
    "    is_notebook = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# we always need to initialzie and define a working directory\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "\n",
    "# if we execute this script as a notebook, the working dir of OGGM will be located in the same folder, named 'working_dir_testing'\n",
    "if is_notebook:\n",
    "    working_dir = 'working_dir_testing'\n",
    "# if we execute this script via slurm, the working dir of OGGM will be copied to the same folder after the slurm run and will be named 'working_dir'\n",
    "else:\n",
    "    working_dir = os.path.join(os.environ[\"OGGM_WORKDIR\"],\n",
    "                               'working_dir'\n",
    "    )\n",
    "\n",
    "cfg.PATHS['working_dir'] = working_dir\n",
    "\n",
    "# use multiprocessing when running the script on the cluster\n",
    "if not is_notebook:\n",
    "    cfg.PARAMS['use_multiprocessing'] = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example script the glaciers for the [goodbye glaciers website](https://goodbye-glaciers.info) are listed, but you can choose any other glacier that are part of the Randolph's Glacier Invetory v6 (this glacier explorer can help you find your glaciers' RGI6 IDs: [GLIMS GLACIER EXPLORER](https://www.glims.org/maps/glims)).\n",
    "If you want to animate neighbouring/nearby glaciers together in one animation, you can define a glacier complex in the `merge_glaciers` dictionary. A complex is defined by its name(used for file naming) and a list of the RGI-IDs that should be part of this complex. The glaciers of a complex also have to be part of the `rgi_ids`-list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rgi_ids = [\n",
    "    'RGI60-11.00897', # Hintereisferner\n",
    "    'RGI60-11.00106', # Pasterze\n",
    "    'RGI60-11.01238', # Rhone\n",
    "    'RGI60-11.03887', # Marmolada\n",
    "    'RGI60-11.03671', # Gebroulaz\n",
    "    'RGI60-11.00593' # Seekarlesferner\n",
    "]\n",
    "# if no glaciers should be merged, set `merge_glaciers` to `None`\n",
    "merge_glaciers = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the gdirs and initializing the glaciers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# loading gdirs from preprocessed directories\n",
    "prepro_base_url_L0 = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/elev_bands/'\n",
    "gdirs = workflow.init_glacier_directories(rgi_ids,\n",
    "                                          from_prepro_level=0,\n",
    "                                          prepro_base_url=prepro_base_url_L0,\n",
    "                                          prepro_border=80,  # could be 10, 80, 160 or 240\n",
    "                                          reset=True,\n",
    "                                          force=True,\n",
    "                                         )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if is_notebook:\n",
    "    gdirs = [gdirs[0]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in the following cell the gridsize(which equals the resolution of the glacier in meters) of the simulation can be set with `cfg.PARAMS['fixed_dx']`. Here it is set to 30 meters, which is already a high resolution for a big glacier in the European Alps(e.g. Aletsch, Hintereisferner..)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# define the border, we keep the default here\n",
    "cfg.PARAMS['border'] = cfg.PARAMS['border']\n",
    "\n",
    "# set the method for determining the local grid resolution\n",
    "cfg.PARAMS['grid_dx_method'] = 'fixed'  # The default method is 'square', which determines the grid spacing (dx) based on the glacier's outline area.\n",
    "cfg.PARAMS['fixed_dx'] = 30  # This allows setting a specific resolution in meters. It's applicable only when grid_dx_method is set to 'fixed'.\n",
    "# set the DEM source to use\n",
    "source = 'COPDEM30'  # we stick with the OGGM default\n",
    "\n",
    "# this task adds the DEM and defines the local grid\n",
    "workflow.execute_entity_task(tasks.define_glacier_region, gdirs,\n",
    "                             source=source);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose type of flowline to use: 'elevation_band' or 'centerline' ([more info on flowline types](https://docs.oggm.org/en/latest/flowlines.html))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "flowline_type_to_use = 'elevation_band'  # you can also select 'centerline' here\n",
    "\n",
    "if flowline_type_to_use == 'elevation_band':\n",
    "    elevation_band_task_list = [\n",
    "        tasks.simple_glacier_masks,\n",
    "        tasks.elevation_band_flowline,\n",
    "        tasks.fixed_dx_elevation_band_flowline,\n",
    "        tasks.compute_downstream_line,\n",
    "        tasks.compute_downstream_bedshape,\n",
    "    ]\n",
    "\n",
    "    for task in elevation_band_task_list:\n",
    "        workflow.execute_entity_task(task, gdirs);\n",
    "\n",
    "elif flowline_type_to_use == 'centerline':\n",
    "    # for centerline we can use parabola downstream line\n",
    "    cfg.PARAMS['downstream_line_shape'] = 'parabola'\n",
    "\n",
    "    centerline_task_list = [\n",
    "        tasks.glacier_masks,\n",
    "        tasks.compute_centerlines,\n",
    "        tasks.initialize_flowlines,\n",
    "        tasks.catchment_area,\n",
    "        tasks.catchment_intersections,\n",
    "        tasks.catchment_width_geom,\n",
    "        tasks.catchment_width_correction,\n",
    "        tasks.compute_downstream_line,\n",
    "        tasks.compute_downstream_bedshape,\n",
    "    ]\n",
    "\n",
    "    for task in centerline_task_list:\n",
    "        workflow.execute_entity_task(task, gdirs);\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown flowline type '{flowline_type_to_use}'! Select 'elevation_band' or 'centerline'!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### massbalance calibration ([more info](https://tutorials.oggm.org/stable/notebooks/tutorials/massbalance_calibration.html))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "source": [
    "# define the climate data to use, we keep the default\n",
    "cfg.PARAMS['baseline_climate'] = cfg.PARAMS['baseline_climate']\n",
    "\n",
    "# add climate data to gdir\n",
    "workflow.execute_entity_task(tasks.process_climate_data, gdirs);\n",
    "\n",
    "# the default mb calibration\n",
    "workflow.execute_entity_task(tasks.mb_calibration_from_geodetic_mb,\n",
    "                             gdirs,\n",
    "                             informed_threestep=True,  # only available for 'GSWP3_W5E5'\n",
    "                            );\n",
    "\n",
    "# glacier bed inversion\n",
    "workflow.execute_entity_task(tasks.apparent_mb_from_any_mb, gdirs);\n",
    "workflow.calibrate_inversion_from_consensus(\n",
    "    gdirs,\n",
    "    apply_fs_on_mismatch=True,\n",
    "    error_on_mismatch=True,  # if you running many glaciers some might not work\n",
    "    filter_inversion_output=True,  # this partly filters the overdeepening due to\n",
    "    # the equilibrium assumption for retreating glaciers (see. Figure 5 of Maussion et al. 2019)\n",
    "    volume_m3_reference=None,  # here you could provide your own total volume estimate in m3\n",
    ");\n",
    "\n",
    "# finally create the dynamic flowlines\n",
    "workflow.execute_entity_task(tasks.init_present_time_glacier, gdirs);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dynamic spinup ([more info](https://tutorials.oggm.org/stable/notebooks/tutorials/dynamical_spinup.html))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cfg.PARAMS['store_fl_diagnostics'] = True\n",
    "\n",
    "# set the ice dynamic solver depending on the flowline-type\n",
    "if flowline_type_to_use == 'elevation_band':\n",
    "    cfg.PARAMS['evolution_model'] = 'SemiImplicit'\n",
    "elif flowline_type_to_use == 'centerline':\n",
    "    cfg.PARAMS['evolution_model'] = 'FluxBased'\n",
    "else:\n",
    "    raise ValueError(f\"Unknown flowline type '{flowline_type_to_use}'! Select 'elevation_band' or 'centerline'!\")\n",
    "\n",
    "# get the start and end year of the selected baseline\n",
    "y0 = gdirs[0].get_climate_info()['baseline_yr_0']\n",
    "ye = gdirs[0].get_climate_info()['baseline_yr_1'] + 1  # run really to the end until 1.1.\n",
    "\n",
    "\n",
    "# 'dynamic' initialisation, including dynamic mb calibration\n",
    "dynamic_spinup_start_year = 1979\n",
    "minimise_for = 'area'  # other option would be 'volume'\n",
    "for gdir in gdirs:\n",
    "    workflow.execute_entity_task(\n",
    "        tasks.run_dynamic_melt_f_calibration, [gdir],\n",
    "        err_dmdtda_scaling_factor=0.2,  # by default we reduce the mass balance error for accounting for\n",
    "        # corrleated uncertainties on a regional scale\n",
    "        ys=dynamic_spinup_start_year, ye=ye,\n",
    "        kwargs_run_function={'minimise_for': minimise_for,\n",
    "                                'precision_percent': 1.3} if gdir.rgi_id == 'RGI60-11.03887' else {'minimise_for': minimise_for},\n",
    "        ignore_errors=True,\n",
    "        kwargs_fallback_function={'minimise_for': minimise_for},\n",
    "        output_filesuffix='_spinup_historical',\n",
    "        \n",
    "    );"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# look at the statistics if everything is working\n",
    "opath = os.path.join(cfg.PATHS['working_dir'], 'glacier_statistics.csv')\n",
    "df_statistics = utils.compile_glacier_statistics(gdirs, path=opath)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if is_notebook:\n",
    "    # if all none all worked without an error\n",
    "    print(df_statistics['error_task'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and process GCM data - 2 options:\n",
    "#### - set `temperature_scenarios` to `False` to use ssp-based clustering of the scenarios \n",
    "#### - set `temperature_scenarios` to `True` to use warming-based clustering of the scenarios \n",
    "#### - the ssp-scenarios and temperature scenarios can be defined further down"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# choose one option by setting the boolean:\n",
    "# - True: wariming-based scenario clustering\n",
    "# - False: ssp-based scenario clustering\n",
    "temperature_scenarios = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load CMIP5 + CMIP6 metadata\n",
    "gcms_cmip6 = pd.read_csv('/home/www/oggm/cmip6/all_gcm_list.csv', index_col=0)\n",
    "gcms_cmip5 = pd.read_csv('/home/www/oggm/cmip5-ng/all_gcm_list.csv', index_col=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option I: SSP scenarios taken from CMIP6\n",
    "#### - If you choose this option, define the ssp scenarios that will be used in the third line (e.g. `scenarios = ['ssp126', 'ssp585']`)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not temperature_scenarios: \n",
    "    # set the ssp scenarios here\n",
    "    scenarios = ['ssp126', 'ssp585']\n",
    "    \n",
    "    def get_models_per_scenario(scenario):\n",
    "        return np.unique(gcms_cmip6[gcms_cmip6.ssp == scenario].gcm.values)\n",
    "    \n",
    "    gcms_per_scenario = {}\n",
    "    for scenario in scenarios:\n",
    "        gcms_per_scenario[scenario] = get_models_per_scenario(scenario)\n",
    "        # if is_notebook:\n",
    "        #     gcms_per_scenario[scenario] = [gcms_per_scenario[scenario][0]]\n",
    "\n",
    "    # iterate through the scenarios        \n",
    "    for scenario in gcms_per_scenario:\n",
    "        for gcm in gcms_per_scenario[scenario]:\n",
    "            rid = f'{gcm}_{scenario}'\n",
    "    \n",
    "            select_gcm = np.array([g.upper() for g in gcms_cmip6.gcm]) == gcm.upper()\n",
    "            select_ssp = gcms_cmip6.ssp == scenario\n",
    "            selected_run = gcms_cmip6[select_gcm & select_ssp]\n",
    "            ft = selected_run[selected_run['var'] == 'tas'].path.values[0]\n",
    "            fp = selected_run[selected_run['var'] == 'pr'].path.values[0]\n",
    "    \n",
    "            ft = utils.file_downloader(ft)\n",
    "            fp = utils.file_downloader(fp)\n",
    "            \n",
    "            # bias correct them\n",
    "            workflow.execute_entity_task(gcm_climate.process_cmip_data, gdirs,\n",
    "                                         year_range=('2000', '2019'),\n",
    "                                         filesuffix=rid,  # recognize the climate file for later\n",
    "                                         fpath_temp=ft,  # temperature projections\n",
    "                                         fpath_precip=fp,  # precip projections\n",
    "                                         );"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option II: Selection of Model runs from the CMIP5 and CMIP6 that show a certain warming towards the end of the century\n",
    "#### - define the temperature scenarios that will be used in the third code line (e.g. `scenarios = [1.5, 2.7]`)\n",
    "#### - define the temperature margins that will be used in the fourth code line (e.g. `temp_margin = 0.25`)\n",
    "#### this will select all GCM runs, from the ones we have available(CMIP5 and CMIP6), for which the \"preindustrial-2100\" global warming value is within the given temperature margin around the given temperature level."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "if temperature_scenarios:\n",
    "    # set the temperatures and the temperature margin\n",
    "    scenarios = [1.5, 2.7] # °C warming from pre-industrial to the year 2100\n",
    "    temp_margin = 0.25 # +/- °C temperature difference from the given scenario value that is still accepted for this scenario\n",
    "    \n",
    "    # load the data containing the temperature difference between pre-industrial and the modelled year 2100\n",
    "    _file_cmip5 = '/home/www/oggm/oggm-standard-projections/analysis_notebooks/Global_mean_temp_deviation_2071_2100_2081_2100_2271_2300_2281_2300_rel_1850_1900_cmip5_gcms_ipcc_ar6_def.csv'\n",
    "    _file_cmip6 = '/home/www/oggm/oggm-standard-projections/analysis_notebooks/Global_mean_temp_deviation_2071_2100_2081_2100_2271_2300_2281_2300_rel_1850_1900_cmip6_gcms_ipcc_ar6_def.csv'\n",
    "    pd_cmip5_temp_ch_2100 = pd.read_csv(_file_cmip5, index_col=0)\n",
    "    pd_cmip5_temp_ch_2100['cmip'] = 'CMIP5'\n",
    "    \n",
    "    pd_cmip6_temp_ch_2100 = pd.read_csv(_file_cmip6, index_col=0)\n",
    "    pd_cmip6_temp_ch_2100['cmip'] = 'CMIP6'\n",
    "    \n",
    "    pd_cmip_temp_ch_2100 = pd.concat([pd_cmip5_temp_ch_2100, pd_cmip6_temp_ch_2100])\n",
    "    pd_cmip_temp_ch_2100.index = list(map(lambda x: x.upper(), pd_cmip_temp_ch_2100.index))\n",
    "    \n",
    "    ## chosen temp...\n",
    "    def get_models_from_temp(temp, lower_limit, upper_limit):\n",
    "        pi_l = temp + lower_limit\n",
    "        pi_u = temp + upper_limit\n",
    "        pd_cmip_sel = pd_cmip_temp_ch_2100.loc[(pd_cmip_temp_ch_2100['global_temp_ch_2071-2100_preindustrial']>=pi_l)&\n",
    "        (pd_cmip_temp_ch_2100['global_temp_ch_2071-2100_preindustrial']<=pi_u)]\n",
    "        return pd_cmip_sel\n",
    "    scenarios_with_gcm = {}\n",
    "    for scenario in scenarios:\n",
    "        scenario_name = '+' + str(scenario) + '°C'\n",
    "        scenarios_with_gcm[scenario_name] = get_models_from_temp(scenario, -abs(temp_margin), +abs(temp_margin))\n",
    "\n",
    "    for scenario in scenarios_with_gcm:\n",
    "        df_gcm = scenarios_with_gcm[scenario]\n",
    "        for index, gcm in df_gcm.iterrows():\n",
    "            if gcm.cmip == 'CMIP5':\n",
    "                select_gcm = np.array([g.upper() for g in gcms_cmip5.gcm]) == gcm.gcm\n",
    "                select_rcp = gcms_cmip5.rcp == gcm.rcp\n",
    "                selected_run = gcms_cmip5[select_gcm & select_rcp]\n",
    "                ft = selected_run[selected_run['var'] == 'tas'].path.values[0]\n",
    "                fp = selected_run[selected_run['var'] == 'pr'].path.values[0]\n",
    "            elif gcm.cmip == 'CMIP6':\n",
    "                select_gcm = np.array([g.upper() for g in gcms_cmip6.gcm]) == gcm.gcm\n",
    "                select_rcp = gcms_cmip6.ssp == gcm.ssp\n",
    "                selected_run = gcms_cmip6[select_gcm & select_rcp]\n",
    "                ft = selected_run[selected_run['var'] == 'tas'].path.values[0]\n",
    "                fp = selected_run[selected_run['var'] == 'pr'].path.values[0]\n",
    "            else:\n",
    "                raise ValueError(gcm.cmip)\n",
    "    \n",
    "            ft = utils.file_downloader(ft)\n",
    "            fp = utils.file_downloader(fp)\n",
    "            # bias correct them\n",
    "            workflow.execute_entity_task(gcm_climate.process_cmip_data, gdirs,\n",
    "                                         year_range=('2000', '2019'),\n",
    "                                         filesuffix=index,  # recognize the climate file for later\n",
    "                                         fpath_temp=ft,  # temperature projections\n",
    "                                         fpath_precip=fp,  # precip projections\n",
    "                                         );\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection runs \n",
    "### The following cells will exectue the actual OGGM model runs, 1 run for each GCM and each Glacier selected"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# these values have to be set to True, so that the flowline evolution over the projected period is saved for each timestep. This data is later on used for the redistribution and finally for the animation of the glaciers.\n",
    "cfg.PARAMS['store_model_geometry'] = True\n",
    "cfg.PARAMS['store_fl_diagnostics'] = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "if temperature_scenarios:\n",
    "    scenario_suffixes = {key: list(value.index) for key, value in scenarios_with_gcm.items()}\n",
    "else:\n",
    "    for ssp_scenario, gcms in gcms_per_scenario.items():\n",
    "        gcms = gcms.astype(str)\n",
    "        gcms_per_scenario[ssp_scenario] = np.char.add(gcms, f\"_{ssp_scenario}\").tolist()\n",
    "        scenario_suffixes = gcms_per_scenario\n",
    "\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now run OGGM forced with various climate scenarios **starting from the end year of the historical spin-up run**:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for scenario in scenario_suffixes.values():\n",
    "    for scenario_suffix in scenario:\n",
    "        workflow.execute_entity_task(tasks.run_from_climate_data, gdirs,\n",
    "                                     climate_filename='gcm_data',  # use gcm_data, not climate_historical\n",
    "                                     climate_input_filesuffix=scenario_suffix,  # use the chosen scenario\n",
    "                                     init_model_filesuffix='_spinup_historical',  # this is important! Start from 2020 glacier\n",
    "                                     output_filesuffix=scenario_suffix,  # recognize the run for later\n",
    "                                    );"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the model median for each scenario\n",
    "This function takes the different gcm runs for one glacier and calculates a median flowline (quantiles = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for scenario, suffix_list in scenario_suffixes.items():\n",
    "    workflow.execute_entity_task(tasks.compute_fl_diagnostics_quantiles,\n",
    "                             gdirs,\n",
    "                             input_filesuffixes=suffix_list,\n",
    "                             quantiles=0.5,\n",
    "                             output_filesuffix=f'_median_{scenario}'\n",
    "                             )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create redistributed data for each glacier and glacier complex\n",
    "### The data that will be generated below is used for the creation of the animations in the next script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_mini_ds()` is a helper function that allows to create a small version of our data for test purposeses."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def mini_ds_generator(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process .nc files in the input directory, reduce to first two timesteps,\n",
    "    and save to the output directory, preserving the directory structure. \n",
    "    This so created mini datasets can be used for test purposes without moving large files.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): The source directory containing the .nc files.\n",
    "        output_dir (str): The destination directory to save the processed .nc files.\n",
    "    \"\"\"\n",
    "    # Walk through the directory structure of the input directory\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.nc'):  # Process only .nc files\n",
    "                input_file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Construct the relative path of the file\n",
    "                relative_path = os.path.relpath(root, input_dir)\n",
    "                output_folder = os.path.join(output_dir, relative_path)\n",
    "                \n",
    "                # Ensure the output folder exists\n",
    "                if not os.path.exists(output_folder):\n",
    "                    os.makedirs(output_folder)\n",
    "                \n",
    "                # Open the .nc file with xarray\n",
    "                ds = xr.open_dataset(input_file_path)\n",
    "                \n",
    "                # Reduce the dataset to the first two timesteps along the 'time' dimension\n",
    "                ds_reduced = ds.isel(time=slice(0, 2))\n",
    "                \n",
    "                # Define the output file path\n",
    "                output_file_path = os.path.join(output_folder, file)\n",
    "                \n",
    "                # Save the reduced dataset as a new .nc file\n",
    "                ds_reduced.to_netcdf(output_file_path)\n",
    "                \n",
    "                # Close the dataset\n",
    "                ds.close()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to create mini-ds, which you can easily copy to your local workspace to test the animation settings: \n",
    "### set `create_mini_ds=True`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "create_mini_ds = True\n",
    "\n",
    "# create animation data folder, if it exists already, keep using/overwriting it\n",
    "redist_data_path = os.path.join(cfg.PATHS['working_dir'], 'redistributed_data')\n",
    "os.makedirs(redist_data_path, exist_ok=True)\n",
    "if create_mini_ds:\n",
    "    mini_ds_path = os.path.join(cfg.PATHS['working_dir'], 'mini_ds')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# This is to add a new topography to the file (smoothed differently)\n",
    "workflow.execute_entity_task(distribute_2d.add_smoothed_glacier_topo,\n",
    "                             gdirs)\n",
    "    # This is to get the bed map at the start of the simulation\n",
    "workflow.execute_entity_task(tasks.distribute_thickness_per_altitude,\n",
    "                             gdirs)\n",
    "    # This is to prepare the glacier directory for the interpolation (needs to be done only once)\n",
    "workflow.execute_entity_task(distribute_2d.assign_points_to_band,\n",
    "                             gdirs)\n",
    "\n",
    "for scenario in scenario_suffixes:\n",
    "    for gdir in gdirs:\n",
    "        input_filesuffix = f'_median_{scenario}'\n",
    "        workflow.execute_entity_task(distribute_2d.distribute_thickness_from_simulation,\n",
    "                                     gdir, \n",
    "                                     input_filesuffix=input_filesuffix, \n",
    "                                     concat_input_filesuffix='_spinup_historical',\n",
    "                                     fl_thickness_threshold=2,\n",
    "                                     rolling_mean_smoothing=8,\n",
    "                                     debug_area_timeseries=True,\n",
    "                                     add_monthly=True,\n",
    "                                     smooth_radius=48\n",
    "                                    )\n",
    "        # copy relevant data to a directory outside of the gdirs.\n",
    "        rgi_dir = os.path.join(redist_data_path, gdir.rgi_id)\n",
    "        os.makedirs(rgi_dir, exist_ok=True)\n",
    "        shutil.copy(gdir.get_filepath('gridded_simulation', filesuffix=input_filesuffix), rgi_dir)\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If defined, the data of (neighboring) glaciers can also be merged to have the time evolution of several glaciers in one file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not is_notebook:\n",
    "    if merge_glaciers:\n",
    "        # merge simulations\n",
    "        for name in merge_glaciers:\n",
    "            print(f'Merging {name}:')\n",
    "            # get gdirs of current selection\n",
    "            gdirs_merging = []\n",
    "            for gdir in gdirs:\n",
    "                if gdir.rgi_id in merge_glaciers[name]:\n",
    "                    gdirs_merging.append(gdir)\n",
    "        \n",
    "            # merge each scenario\n",
    "            for scenario in list(scenario_suffixes.keys()):\n",
    "                print(f'   {scenario}')\n",
    "                distribute_2d.merge_simulated_thickness(\n",
    "                    gdirs_merging,  # the gdirs we want to merge\n",
    "                    simulation_filesuffix=f'_median_{scenario}',  # the name of the simulationinput_filesuffix\n",
    "                    output_filename=f'gridded_simulation_merged_{name}_median_{scenario}',\n",
    "                    output_folder=redist_data_path,\n",
    "                    years_to_merge=None,  # for demonstration I only pick some years, if this is None all years are merged\n",
    "                    add_topography='COPDEM30',  # if you do not need topogrpahy setting this to False will decrease computing time\n",
    "                    preserve_totals=True,  # preserve individual glacier volumes during merging\n",
    "                    reset=True,\n",
    "                    save_as_multiple_files=False,\n",
    "                    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if create_mini_ds:\n",
    "    mini_ds_generator(redist_data_path, mini_ds_path)\n",
    "            \n",
    "            "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'FINISHED. All redistributed thickness files are saved at \\'{redist_data_path}\\'.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data created above can now be used to create 3D animations. See [next tutorial](https://www.lipsum.com/). "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
